{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science\n",
    "## From correlation to supervised segmentation and tree-structured models\n",
    "\n",
    "Spring 2018 - Prof. Foster Provost\n",
    "\n",
    "Assistant Lecturer: Nicholas Garcia\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to need a lot of Python **packages**, so let's start by importing all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from IPython.display import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also going to do a lot of repetitive stuff, so let's predefine some useful **FUNCTIONS**:\n",
    "\n",
    "Remember the basic aspects of a function:\n",
    "\n",
    "* Input -> Parameters\n",
    "* Actions\n",
    "* Output -> Return\n",
    "\n",
    "(We might have no _return_ which is basically just an action usually known as procedure. Also, We will see in other classes that these functions can also be taken from a **SCRIPT !!!**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A function that gives a visual representation of the decision tree\n",
    "\n",
    "def Decision_Tree_Image(decision_tree, feature_names, class_names, name=\"tree\",proportion=True):\n",
    "    \n",
    "    # Export our decision tree to graphviz format\n",
    "    dot_file = tree.export_graphviz(decision_tree, out_file='images/' + name + '.dot', \n",
    "                                    feature_names=feature_names, class_names=class_names,proportion=proportion)\n",
    "        \n",
    "    # Call graphviz to make an image file from our decision tree\n",
    "    os.system(\"dot -Tpng images/\" + name + \".dot -o images/\" + name + \".png\")\n",
    "    # to get this part to actually work, you may need to open a terminal window in Jupyter and run the following command \"sudo apt install graphviz\"\n",
    "    \n",
    "    # Return the .png image so we can see it\n",
    "    return Image(filename='images/' + name + '.png')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# A function to plot the data\n",
    "\n",
    "def Plot_Data(data, v1, v2, tv):\n",
    "    # Make the plot square\n",
    "    plt.rcParams['figure.figsize'] = [12.0, 8.0]\n",
    "    \n",
    "    # Color\n",
    "    color = [\"red\" if x == 0 else \"blue\" for x in data[tv]]\n",
    "    \n",
    "    # Plot and label\n",
    "    plt.scatter(data[v1], data[v2], c=color, s=50)\n",
    "    plt.xlabel(v1)\n",
    "    plt.ylabel(v2)\n",
    "    plt.xlim([min(data[v1]) , max(data[v1]) ])\n",
    "    plt.ylim([min(data[v2]) , max(data[v2]) ])\n",
    "    \n",
    "\n",
    "    \n",
    "# A function that creates the surface of a decision tree\n",
    "\n",
    "def Decision_Surface(data, target, model):\n",
    "    # Get bounds\n",
    "    x_min, x_max = data[data.columns[0]].min(), data[data.columns[0]].max()\n",
    "    y_min, y_max = data[data.columns[1]].min(), data[data.columns[1]].max()\n",
    "    \n",
    "    # Create a mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max,0.01), np.arange(y_min, y_max,0.01))\n",
    "    meshed_data = pd.DataFrame(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    plt.figure(figsize=[12,7])\n",
    "    Z = model.predict(meshed_data).reshape(xx.shape)\n",
    "            \n",
    "    plt.title(\"Decision surface\")    \n",
    "    plt.ylabel(\"humor\")\n",
    "    plt.xlabel(\"number_pets\")\n",
    "    cs = plt.contourf(xx, yy, Z, levels=[-1,0,1],cmap=plt.cm.coolwarm,origin='lower')\n",
    "\n",
    "    color = [\"blue\" if t == 0 else \"red\" for t in target]\n",
    "    plt.scatter(data[data.columns[0]], data[data.columns[1]], color=color )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need some data, so let's create a dataset consisting of **500** people (rows) with **3** different variables (columns): `[\"age\", \"humor\", \"number_pets\"]` \n",
    "\n",
    "The **target** of our prediction will be whether or not a person feels successful. We will call it \"success\" ( **binary** -> 0/1 values )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the randomness\n",
    "np.random.seed(36)\n",
    "\n",
    "# Number of users\n",
    "n_users = 500\n",
    "\n",
    "# Relationships\n",
    "variable_names = [\"age\", \"humor\", \"number_pets\"]\n",
    "variables_keep = [\"number_pets\", \"humor\"]\n",
    "target_name = \"success\"\n",
    "\n",
    "# Generate data with the \"datasets\" function from SKLEARN (package)\n",
    "# This function returns two variables: predictors and target\n",
    "\n",
    "predictors, target = datasets.make_classification(n_features=3, n_redundant=0, \n",
    "                                                  n_informative=2, n_clusters_per_class=2,\n",
    "                                                  n_samples=n_users)\n",
    "\n",
    "# We will write this data in a dataframe (pandas package)\n",
    "\n",
    "data = pd.DataFrame(predictors, columns=variable_names)\n",
    "\n",
    "# We want to take each column of the dataframe to change the values \n",
    "\n",
    "data['age'] = data['age'] * 10 + 50\n",
    "data['humor'] = data['humor'] * 10 + 50\n",
    "data['number_pets'] = (data['number_pets'] + 6)/2\n",
    "data[target_name] = target\n",
    "\n",
    "# Our variables (features) will be stored in one variable called X\n",
    "X = data[[variables_keep[0], variables_keep[1]]]\n",
    "\n",
    "# Our target will be stored in one variable called Y\n",
    "Y = data[target_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can (and should) take a look at the first few rows/records of our data to see what we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last 5 values\n",
    "#X.describe()\n",
    "X.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable: success\n",
    "\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One feature and splits\n",
    "Let's take a look at one of our features -- **`\"number_pets\"`**. Is this feature useful? \n",
    "\n",
    "- Let's look at a scatter plot the possible values of `\"number_pets\"` and color code our target variable, `\"success\"`. Red dots mean unsuccessful people and blue dots mean successful people. \n",
    "- The horizontal value is the number of pets they have. The vertical position here doesn't mean anything (everyone has a one). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['figure.figsize'] = [15.0, 2.0]\n",
    "\n",
    "color = [\"red\" if x == 0 else \"blue\" for x in data[\"success\"]]\n",
    "\n",
    "plt.scatter(X['number_pets'], Y, c=color, s=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Can we say that a number of pets below 3 is a good point to differentiate succesful?  \n",
    "\n",
    "Let's quantify it.\n",
    "\n",
    "\n",
    "**Entropy** ($H$) and **information gain** ($IG$) can help determine which features are the most informative. These metrics are used with categorical (i.e. discrete-valued) variables. Continuous (e.g. numeric) variables can be converted into categories by choosing thresholds along which to split the feature.\n",
    "\n",
    "<table style=\"border: 0px\">\n",
    "<tr style=\"border: 0px\">\n",
    "<td style=\"border: 0px\"><img src=\"images/dsfb_0304.png\" height=80% width=80%>\n",
    "Figure 3-4. Splitting the \"write-off\" sample into two segments, based on splitting the Balance attribute (account balance) at 50K.</td>\n",
    "<td style=\"border: 0px; width: 30px\"></td>\n",
    "<td style=\"border: 0px\"><img src=\"images/dsfb_0305.png\" height=75% width=75%>\n",
    "Figure 3-5. A classification tree split on the three-values Residence attribute.</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions to get the entropy and IG\n",
    "Entropy and information gain are both relatively easy to calculate using the method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def entropy(target):\n",
    "    # Get the number of instances\n",
    "    n = len(target)\n",
    "    # Count how frequently each unique target value occurs using the numpy function \n",
    "    counts = np.bincount(target).astype(float)\n",
    "    # Initialize entropy\n",
    "    entropy = 0\n",
    "    \n",
    "    # Otherwise, for each possible value, update entropy; use zero for 0 log 0\n",
    "    for count in counts:\n",
    "        if count == 0:\n",
    "            entropy += 0\n",
    "        else:\n",
    "            entropy += math.log(count/n, 2) * count/n\n",
    "    # Return entropy\n",
    "    return -1 * entropy\n",
    "\n",
    "def information_gain(feature, threshold, target):\n",
    "    # Dealing with numpy arrays makes this slightly easier\n",
    "    target = np.array(target)\n",
    "    feature = np.array(feature)\n",
    "    # Cut the feature vector on the threshold\n",
    "    feature = (feature < threshold)\n",
    "    # Initialize information gain with the parent entropy\n",
    "    ig = entropy(target)\n",
    "    # For both sides of the threshold, update information gain\n",
    "    for level, count in zip([0, 1], np.bincount(feature).astype(float)):\n",
    "        ig -= count/len(feature) * entropy(target[feature == level])\n",
    "    # Return information gain\n",
    "    return ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['figure.figsize'] = [15.0, 2.0]\n",
    "\n",
    "color = [\"red\" if x == 0 else \"blue\" for x in data[\"success\"]]\n",
    "\n",
    "plt.scatter(X['number_pets'], Y, c=color, s=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we thought that 3 pets looked like a good cutoff point? let's see how much information gain ($IG$) we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "threshold = 3\n",
    "print (\"IG = %.4f with thresholding of %.2f.\" % (information_gain(X['number_pets'], threshold, np.array(Y)), threshold))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be more precise, we can iterate through all values and find the best split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def best_threshold():\n",
    "    maximum_ig = 0\n",
    "    maximum_threshold = 0\n",
    "\n",
    "    for threshold in X['number_pets']:\n",
    "        ig = information_gain(X['number_pets'], threshold, np.array(Y))\n",
    "        if ig > maximum_ig:\n",
    "            maximum_ig = ig\n",
    "            maximum_threshold = threshold\n",
    "\n",
    "    return \"The maximum IG = %.3f and it occured by splitting on %.4f.\" % (maximum_ig, maximum_threshold)\n",
    "\n",
    "print ( best_threshold() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features and splits with the sklearn package !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can do this with just sklearn! Now, we will be using all the variables in X, not only number of pets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[7,5])\n",
    "Plot_Data(data, \"number_pets\",  \"humor\",\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the model (tree)\n",
    "my_tree = DecisionTreeClassifier(max_depth=10,criterion=\"entropy\")   # Look at those 2 arguments !!! \n",
    "\n",
    "# Let's tell the model what is the data\n",
    "my_tree.fit(X, Y)\n",
    "\n",
    "#Let's print an image with the results\n",
    "Decision_Tree_Image(my_tree, X.columns, class_names =['fail','success'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `\"humor\"`and `\"number_pets\"`, including the **DECISION SURFACE!!**\n",
    "\n",
    "More details for this graph: [sklearn decision surface](http://scikit-learn.org/stable/auto_examples/tree/plot_iris.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decision_Surface(X,Y,my_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ( \"Accuracy = %.3f\" % (metrics.accuracy_score(my_tree.predict(X), Y)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
