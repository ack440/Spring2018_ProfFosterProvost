{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science\n",
    "## Fitting models and overfitting  + using scripts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading & Installing Packages\n",
    "Enter the following commands into jupyter's terminal:\n",
    "```\n",
    "sudo pip install liac-arff\n",
    "sudo apt install graphviz\n",
    "sudo pip install graphviz\n",
    "```\n",
    "enter \"`y`\" when promted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "from dstools import data_tools\n",
    "\n",
    "\n",
    "# for plotting\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "import pydotplus\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as pylab\n",
    "pylab.rcParams['figure.figsize'] = 14, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scripts and Data\n",
    "\n",
    "How can we use a script?  Let's use a script to create our artificial dataset, rather than putting the code into the notebook.  That way the code doesn't clutter up the notebook. I think about it this way: I use all sorts of Python functions, Pandas functions, and so on.  I don't care to have the code for how those functions work in my notebook.  And for the most part, I don't ever want to see that code, as long as I understand the functional (input->output) behavior (and any important side-effects).  \n",
    "\n",
    "However, there are more important reasons to use such scripts--similar to the reasons for using other packages.  We can now use our new code across different notebooks.  Moreover, when we fix or improve the code in the script, it is fixed across all the notebooks.  \n",
    "\n",
    "[Of course, with such power comes responsibility.  If the code is used across multiple notebooks, it is important to make sure that you keep the input/output functionality of the code the same, so we don't screw up something we did in a prior notebook.]\n",
    "\n",
    "Take a look at the following:\n",
    "\n",
    "* We use the folder **_dstools_** that is in the same directory (folder) as this notebook\n",
    "* We import the file: **data\\_tools**\n",
    "\n",
    "This file is a   \".py\" which has Python commands and functions:\n",
    "\n",
    "1. Decision_Surface -- this is the function that visualizes the segmentation of the learned model\n",
    "2. create_data -- this creates the artificial data set, for us to experiment with\n",
    "3. X -- pulls out the features from the artificial data set just created\n",
    "\n",
    "After the \"import\" we can use these 3 functions, just like we use pre-defined packages like Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some data: The create_data() function returns 4 variables:\n",
    "target_name, variable_names, data, Y = data_tools.create_data()\n",
    "\n",
    "# Grab the predictors (rows and columns)\n",
    "X = data_tools.X()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now put a portion of the data into a Pandas DataFrame to take a look at it\n",
    "pd.DataFrame(list(zip(X.head(10)['humor'],X.head(10)['number_pets'],Y.head(10))),\n",
    "             columns=['humor','number_pets','success'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our data visually, rather than as a table. \n",
    "\n",
    "When we have only two (numeric) features, a scatterplot using these as the axes represents the \"space\" of instances. We can visualize how the target is distributed by representing the target of each instance (point) with a different marker.  We will use color.\n",
    "\n",
    "\n",
    "Our two features are `humor` and `number_pets`. We will visualize whether or not there seems to be a pattern of which users are `success`ful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,7])\n",
    "data_tools.Decision_Surface(X, Y, None, surface=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-structured models\n",
    "Let's now re-explore the modeling technique we introduced last class -- tree-structured models.  And in particular, classification trees (since our target variable is categorical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "depths = [1,2,3,4,5,10] \n",
    "show_probabilities = True\n",
    "\n",
    "\n",
    "nrows = np.floor(np.sqrt(len(depths)))\n",
    "ncol = 2 if len(depths) == 4 else 3\n",
    "plt.figure(figsize=[15,7*nrows])\n",
    "\n",
    "position = 1\n",
    "for depth in depths:\n",
    "\n",
    "    # Model\n",
    "    model = DecisionTreeClassifier(max_depth=depth)\n",
    "    model.fit(X, Y) \n",
    "    \n",
    "    # Plot\n",
    "\n",
    "    plt.subplot(nrows, ncol, position)\n",
    "    position += 1\n",
    "    data_tools.Decision_Surface(X, Y, model, probabilities=show_probabilities)\n",
    "    plt.title(\"Decision Tree Classifier (max depth=\" + str(depth) + \")\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trees are non-linear models\n",
    "\n",
    "If you experiment with the tree depth, you will see that you can fit the data better and better. Deeper trees produce chop the instance space into smaller and smaller pieces.  Try it above, using the `depths` variable.  (Will this finer and finer segmentation go on forever?)\n",
    "\n",
    "**Extra:** Can you visualize the actual tree-structured model?  Hint: there's a function to do it in last week's notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear discriminant models\n",
    "\n",
    "Chapter 4 introduces linear models.  Let's try building one on this data set. \n",
    "\n",
    "Looking at the data (see scatterplot above), can you estimate by eye where a good linear discriminant would be?\n",
    "\n",
    "We will build a **Logistic regression** model. You can also find logistic regression modeling in the sklearn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "show_probabilities = False\n",
    "\n",
    "# Model\n",
    "model = LogisticRegression()\n",
    "model.fit(X, Y)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=[10,7])\n",
    "data_tools.Decision_Surface(X, Y, model, probabilities=show_probabilities)\n",
    "plt.title(\"Linear model\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Probabilities\n",
    "\n",
    "Ok.  For many business problems, we don't need just to estimate the categorical target variable, but we want to estimate the probability that a particular value will be taken.  Just about every classification model can also tell you the estimated probability of class membership.  \n",
    "\n",
    "Intuitively, how would you generate probabilities from a classification tree?  From a linear discriminant? \n",
    "\n",
    "Let's go back and look at the probabilities estimated by these models. You can visualize the probabilities both for the linear model and the tree-structured model. You can do this by modifying the settings at the top of each code block above **(`show_probabilities = True` or `False`)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear models\n",
    "\n",
    "We saw that tree-structured models can fit the data very well.  It seems like a linear model possibly cannot.  Can we use the idea of fitting linear models to generate non-linear boundaries with **logistic regression**? \n",
    "\n",
    "Yes! We can do this by adding non-linear features, such as $humor^2$ or $humor^3$. \n",
    "\n",
    "_** This is one of the most common ways of introducing non-linearity into numeric function modeling: use a linear function learner, but introduce non-linear features.**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "max_order = 3  # Maximum of 3 was created in the script:  data_tools.py\n",
    "\n",
    "show_probabilities = True\n",
    "plt.figure(figsize=[15,7])\n",
    "\n",
    "nrows = np.floor(np.sqrt(max_order))\n",
    "ncol = 2 if max_order == 4 else 3\n",
    "plt.figure(figsize=[15,7*nrows])\n",
    "\n",
    "for order in range(1, max_order+1):\n",
    "    # Get a dataset X_complex with non linear variables\n",
    "    X_complex = data_tools.X(order)\n",
    "    \n",
    "    # Model used to predict\n",
    "    model = LogisticRegression(penalty='l2')\n",
    "    model.fit(X_complex, Y)\n",
    "    \n",
    "    # Plot and calculate accuracy\n",
    "    plt.subplot(nrows, ncol, order)\n",
    "    data_tools.Decision_Surface(X_complex, Y, model, probabilities=show_probabilities)\n",
    "    acc_value = metrics.accuracy_score(model.predict(X_complex), Y) \n",
    "    plt.title(\"Linear model \" + str(order) + \"-order (accuracy: \"+ str(round(acc_value,3))+\")\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "penalties = [1,2,3,4]\n",
    "ncol = 2\n",
    "nrows = np.ceil(len(penalties)/ncol)\n",
    "plt.figure(figsize=[15,7*nrows])\n",
    "\n",
    "for inv_penalty in penalties:\n",
    "    # Get a dataset X_complex with non linear variables\n",
    "    X_complex = data_tools.X(max_order)\n",
    "    \n",
    "    # Model used to predict\n",
    "    model = LogisticRegression(penalty='l1',C=10**(-inv_penalty))\n",
    "    model.fit(X_complex, Y)\n",
    "    \n",
    "    # Plot and calculate accuracy\n",
    "\n",
    "    plt.subplot(nrows, ncol,idx)\n",
    "    idx = idx +1\n",
    "    data_tools.Decision_Surface(X_complex, Y, model, probabilities=show_probabilities)\n",
    "    acc_value = metrics.accuracy_score(model.predict(X_complex), Y) \n",
    "    plt.title(\"Linear model \" + str(order) + \"-order, L1 penalty: 10^\"+str(inv_penalty)+\"  (accuracy: \"+ str(round(acc_value,3))+\")\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "So, what does the data look like with the non-linear features? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_complex.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model is better in this case?? Look at the **accuracy** of each one.   Accuracy is simply the count of correct decisions divided by the total number of decisions.\n",
    "\n",
    "[From sklearn documentation on sklearn.metrics.accuracy_score: \"In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.\"  [More about the accuracy measure..](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)]\n",
    "\n",
    "Of course, we can also look at the **probabilities** on these non-linear surfaces. Try it out above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization\n",
    "\n",
    "Our evaluation above actually was not what we really want.\n",
    "\n",
    "What we want are models that **generalize** to data that were not used to build them! In other words, we want this model to be able to predict the target for new data instances! Do we know how well our models generalize? Why is this important?\n",
    "\n",
    "<img src=\"images/generalization.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "Let's apply this concept to our data. Now, before we fit out models, we set aside some data to be used later for testing ('holdout data').  This allows us to assess whether the model simply fit the training dataset well, or whether it truly fit some regularities in the domain. \n",
    "\n",
    "Let's use sklearn to set aside some randomly selected holdout data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set randomness so that we all get the same answer\n",
    "np.random.seed(842)\n",
    "\n",
    "# Split the data into train and test pieces for both X and Y\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split our data, let's revisit the tree-structured classifier. Let's check how well a model does when it is fit on a training set and then used to predict on both the training set as well as our holdout set. Remember, the model has never seen this holdout \"test\" set before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print ( \"Accuracy on training = %.4f\" % metrics.accuracy_score(model.predict(X_train), Y_train) )\n",
    "print ( \"Accuracy on test = %.4f\" % metrics.accuracy_score(model.predict(X_test), Y_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results on the test set were worse. Why is this? Can it ever do beter?\n",
    "\n",
    "What happens as our tree gets more and more complicated?  (Deeper and deeper.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_train = []\n",
    "accuracies_test = []\n",
    "maxdepth = 20\n",
    "depths = range(1, maxdepth+1)\n",
    "\n",
    "plt.figure(figsize=[10,7])\n",
    "\n",
    "for md in depths:\n",
    "    model = DecisionTreeClassifier(max_depth=md)\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    accuracies_train.append(metrics.accuracy_score(model.predict(X_train), Y_train))\n",
    "    accuracies_test.append(metrics.accuracy_score(model.predict(X_test), Y_test))\n",
    "\n",
    "plt.plot(depths, accuracies_train, label=\"Train\")\n",
    "plt.plot(depths, accuracies_test, label=\"Test\")\n",
    "plt.title(\"Performance on train and test data\")\n",
    "plt.xlabel(\"Max depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([min(accuracies_test), 1.0])\n",
    "plt.xlim([1,maxdepth])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Above, we made a single train/test split. We set aside 20% of our data and *never* used it for training. We also never used the 80% of the data set aside for training to test generalizability.  Although this is far better than testing on the training data, which does not measure generalization performance at all, there are two potential problems with the simple holdout approach.\n",
    "\n",
    "1) Perhaps the random split was particularly bad (or good).  Do we have any confidence in our accuracy estimate?\n",
    "\n",
    "2) We are using only 20\\% of the data for testing.  Could we possibly use the data more fully for testing?\n",
    "\n",
    "Instead of only making the split once, let's use \"cross-validation\" -- every record will contribute to testing as well as to training.\n",
    "\n",
    "\n",
    "<img src=\"images/cross.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = DecisionTreeClassifier(max_depth=1)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print ( \"Cross validation accuracy on training = %.3f\" % np.mean(cross_val_score(model, X, Y)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add this to our plot from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracies_cross_validation = []\n",
    "depths = range(1, 21)\n",
    "\n",
    "plt.figure(figsize=[10,7])\n",
    "\n",
    "for md in depths:\n",
    "    model = DecisionTreeClassifier(max_depth=md)\n",
    "    \n",
    "    accuracies_cross_validation.append(np.mean(cross_val_score(model, X, Y,cv=12)))\n",
    "\n",
    "plt.plot(depths, accuracies_train, label=\"Train\")\n",
    "plt.plot(depths, accuracies_test, label=\"Test\")\n",
    "plt.title(\"Performace on train and test data\")\n",
    "plt.plot(depths, accuracies_cross_validation, label=\"Cross Validation\")\n",
    "plt.xlabel(\"Max depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([min(accuracies_cross_validation) , 1.0])\n",
    "plt.xlim([1,20])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why might the cross valdation score be so much higher than the train and test scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hands-On Section (HW3)\n",
    "Import the churn training and test data in the data folder with pandas' read_csv function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo pip install liac-arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arff\n",
    "arff_train = arff.load(open('data/churn2_train.arff'))\n",
    "train = pd.DataFrame(arff_train['data'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's give the columns their proper names\n",
    "a = arff_train['attributes']\n",
    "train.columns = [i[0] for i in a] \n",
    "train.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Decision Tree\n",
    "In Weka we used the J48 algorithm that implements the C4.5 algorithm. Sklearn instead uses the CART algorithm which does not work with categorical or nominal data. Let us convert each of these categorical variables to numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the values present in each of the categorical columns (float)\n",
    "print(train['COLLEGE'].unique())\n",
    "print(train['REPORTED_SATISFACTION'].unique())\n",
    "print(train['REPORTED_USAGE_LEVEL'].unique())\n",
    "print(train['CONSIDERING_CHANGE_OF_PLAN'].unique())\n",
    "print(train['LEAVE'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# give these all numeric labels now: \n",
    "\n",
    "# create a dictionary mapping each string to a value\n",
    "to_replace = {'COLLEGE':{'one':1,'zero':0},\n",
    "           'REPORTED_SATISFACTION':{'very_unsat':-2,'unsat':-1,'avg':0,'sat':1,'very_sat':2},\n",
    "           'REPORTED_USAGE_LEVEL':{'very_little':-2,'little':-1,'avg':0,'high':1,'very_high':2},\n",
    "           'CONSIDERING_CHANGE_OF_PLAN':{'no':-2,'never_thought':-1,'perhaps':0,'considering':1,'actively_looking_into_it':2},\n",
    "           'LEAVE':{'LEAVE':1,'STAY':0}\n",
    "          }\n",
    "\n",
    "train = train.replace(to_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train[\"LEAVE\"]\n",
    "X_train = train.loc[:, train.columns != \"LEAVE\"]\n",
    "\n",
    "my_tree = DecisionTreeClassifier()\n",
    "my_tree.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import the test data and transform it in the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arff_test = arff.load(open('data/churn2_test.arff'))\n",
    "test = pd.DataFrame(arff_test['data'])\n",
    "a = arff_test['attributes']\n",
    "test.columns = [i[0] for i in a] \n",
    "test = test.replace(to_replace)\n",
    "Y_test = test[\"LEAVE\"]\n",
    "X_test = test.loc[:, test.columns != \"LEAVE\"]\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable #1: Accuracy and Cross-validation\n",
    "Report the accuracy of the classifier on:\n",
    "- the training data\n",
    "- the test set churn2_test.arff\n",
    "- 10-fold cross-validation\n",
    "- 66% train 34% split of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy on the training data:\n",
    "acc_train = my_tree.score(X_train, Y_train)\n",
    "\n",
    "# accuracy on the test data:\n",
    "acc_test = .99\n",
    "\n",
    "# accuracy on 10-fold CV:\n",
    "# hint: use model_selection.cross_val_score() with cv = 10 \n",
    "acc_10cv = .98\n",
    "\n",
    "# accuracy on 66% split:\n",
    "acc_66pct = .99\n",
    "\n",
    "print(\"accuracy on the training data : \" + str(acc_train))\n",
    "print(\"accuracy on the test data     : \" + str(acc_test))\n",
    "print(\"accuracy on the 10-fold CV    : \" + str(acc_10cv))\n",
    "print(\"accuracy on the 66% split     : \" + str(acc_66pct))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable #2: Construct a Fitting Curve\n",
    "One way to control the complexity is by setting the depth as shown above. Another way, as shown in class is by choosing different values for the minimum number of training instances in each leaf of the tree. In sklean's DecisionTreeClassifier this parameter is called \"min_samples_leaf\".\n",
    "\n",
    "- Plot a fitting curve for the train set, test set, and 3-fold cross validation with a range of values for max_depth to find the optimal complexity. \n",
    "- Next, repeat the above with a range of values for **min_samples_leaf** to find the optimal complexity. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable #2.1: Evaluation\n",
    "Based on the above graphs:\n",
    "- What value would you use for max depth?\n",
    "- What value would you use for min leaf size?\n",
    "- Which complexity parameter would you choose (max depth or min leaf size)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "Your own work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves\n",
    "Learning curves determine how much data you realistically need to train your model.\n",
    "* Randomize the rows\n",
    "* Use train_test_split () to select a range of percentages of the training data to use to fit the model\n",
    "* Plot training size vs. accuracy on the test, train, and 3-fold cv\n",
    "\n",
    "The X-axis of your chart should be \"number for instances used\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable #3.1: Evaluation\n",
    "Would you recommend your firm spend money to collect data on more customers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "Your own work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable #3: Interpreting Trees\n",
    "Build and show a tree that is small enough to plot.\n",
    "(Use the package graphviz to plot the tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which attribute is most informative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain in 2-3 sentances why that feature was chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
