{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science\n",
    "## Unsupervised methods\n",
    "***\n",
    "\n",
    "Spring 2018 - Prof. Foster Provost\n",
    "\n",
    "Teacher Assistant: Nicholas Garcia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "plt.rcParams['figure.figsize'] = 17,12\n",
    "\n",
    "np.random.seed(36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning \n",
    "As usual, we will start by reading in some data. Today we will be looking at movie rating data that was derived from Tweets (https://github.com/sidooms/MovieTweetings). The data set consists of three files: `movies.dat`, `ratings.dat`, and `users.dat`. \n",
    "\n",
    "It was created to help research on 'ratings used by recommender systems' http://crowdrec2013.noahlab.com.hk/papers/crowdrec2013_Dooms.pdf\n",
    "\n",
    "We will only be looking at the first two files. Both of them are located in the `data` directory. The _novies_ file includes information per movie (one row per movie), and the _ratings_ file includes one or more ratings per movie. Both files have the columns **movie_id**.\n",
    "\n",
    "These files **do not** have a header row at the top so we will manually set it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Read in the movies data\n",
    "#movies = pd.read_csv(\"data/movies.dat\", names=['movie_id', 'movie_title', 'genre'], \n",
    "#                      encoding='utf-8', sep=\"\\:\\:\", engine='python')\n",
    "\n",
    "### Movie ids don't start at 0 and some are missing, let's remap\n",
    "#movie_id_map = dict(zip(np.argsort(movies['movie_id'].unique())*-1, movies['movie_id'].unique()))\n",
    "\n",
    "### Given the mapping, let's replace the values\n",
    "#movies = movies.replace({\"movie_id\": {v: k for k, v in movie_id_map.items()}})\n",
    "#movies['movie_id'] = movies['movie_id'] * -1\n",
    "\n",
    "#movies.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Read in the ratings data\n",
    "#ratings = pd.read_csv(\"data/ratings.dat\", names=['user_id', 'movie_id', 'rating', 'rating_timestamp'], \n",
    "#                      sep=\"\\:\\:\", engine='python')\n",
    "\n",
    "### User ids start at 1, let's bump them all down by 1\n",
    "#ratings['user_id'] = ratings['user_id'] - 1\n",
    "\n",
    "### Make movie ids match the ones from our movie's data\n",
    "#ratings = ratings.replace({\"movie_id\": {v: k for k, v in movie_id_map.items()}})\n",
    "#ratings['movie_id'] = ratings['movie_id'] * -1\n",
    "\n",
    "### Put our mapping back in order\n",
    "#movie_id_map = dict((key*-1, value) for (key, value) in movie_id_map.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our two data files loaded in and cleaned, we can create one data frame that **joins** both of them together. \n",
    "\n",
    "Function:  ** _merge(left,right, how='inner', on=None,...)_ **\n",
    "\n",
    "This 'how' parameter is similar to SQL [details here](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html)\n",
    "\n",
    "- left: use only keys from left frame (SQL: left outer join)\n",
    "- right: use only keys from right frame (SQL: right outer join)\n",
    "- outer: use union of keys from both frames (SQL: full outer join)\n",
    "- inner: use intersection of keys from both frames (SQL: inner join) - default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#movies_ratings = pd.merge(movies, ratings, on=\"movie_id\").drop(['genre', 'rating_timestamp'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#movies_ratings.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have only one dataframe with titles and more than one rating per movie. Let's see one of the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#movies_ratings [ movies_ratings.movie_id == 22725 ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are **many users rating many movies**, this data can be represented as a movie-by-user matrix (kind of like the document-by-word matrices we saw in our text class). In this new matrix, we can let movies be the rows, users are the columns, and each cell is filled in with a rating.\n",
    "\n",
    "We'll do this with the \"scipy\" library and the sparse module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data = sparse.csr_matrix((movies_ratings['rating'], (movies_ratings['movie_id'], movies_ratings['user_id'])), \n",
    "#                         shape=(max(movies_ratings['movie_id'])+1, max(movies_ratings['user_id'])+1))\n",
    "\n",
    "#### Format: rating in pairs of (movie, user) \n",
    "\n",
    "#data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will not fit in EC2, so I'm going to dump this to a **pickle** object and let you read it in.\n",
    "\n",
    "What is a pickle?? https://docs.python.org/2/library/pickle.html\n",
    "\n",
    "\" The data format used by pickle is Python-specific. This has the advantage that there are no restrictions imposed by external standards such as JSON or XDR (which canâ€™t represent pointer sharing); however it means that non-Python programs may not be able to reconstruct pickled Python objects. By default, the pickle data format uses a relatively compact binary representation. If you need optimal size characteristics, you can efficiently compress pickled data. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#with open('data/movies_ratings.pickle', 'wb') as f:\n",
    "#    pickle.dump(data, f)\n",
    "#    f.close()\n",
    "\n",
    "#with open('data/movies_clean.pickle', 'wb') as f:\n",
    "#    pickle.dump(movies, f)\n",
    "#    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data from pickle (EC2)\n",
    "\n",
    "Let's read the cleaned data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pandas.indexes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6f10fb2d2be3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/movies_clean.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmovies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pandas.indexes'"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('data/movies_ratings.pickle', 'rb') as f:\n",
    "    data = pickle.load(f,encoding='latin1')\n",
    "    f.close()\n",
    "\n",
    "with open('data/movies_clean.pickle', 'rb') as f:\n",
    "    movies = pickle.load(f,encoding='latin1')\n",
    "    f.close()\n",
    "    \n",
    "print (\"DATA REPRESENTATION: \")\n",
    "print (\"\\n (Movie,User) Rating \\n\")\n",
    "print (data[0:5])\n",
    "print (\"\\n Movies information \\n\")\n",
    "print (movies.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction\n",
    "\n",
    "This matrices have many dimensions. How can we predict _likeliness of a movie_ by a particular customer using this data?  How can we find 'similar' movies? \n",
    "\n",
    "We would like to have a low-dimensional representation of the original customer-product space and then compute\n",
    "neighborhood in the reduced space. If, for example, we wanted to see/plot this data into a 2 dimensional space, we would have to find some way of reducing are large number of dimensions (**a.k.a. Matrix decompositions). \n",
    "\n",
    "One way of doing this is with **SVD = Singular Value Decomposition**.\n",
    "\n",
    "[More info here](http://scikit-learn.org/stable/modules/decomposition.html#lsa)\n",
    "\n",
    "This decomposition is basically a factorization of 1 matrix into 3 matrices that provides the best lower rank approximation. One of those matrices is the \"diagonal\" with non-negative real numbers, it has the singular values (ordered) so that the upper left diagonal element of contains the largest singular value. \n",
    "\n",
    "SVD applications:\n",
    "\n",
    "- To compute a pseudoinverse (e.g. to solve linear least squares problems).\n",
    "- Use for principal component analysis \n",
    "- Signal processing and pattern recognition\n",
    "- Dimensionality Reduction for recommendation systems \n",
    "- .. and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D dimensional space\n",
    "\n",
    "D = 2\n",
    "\n",
    "svd = TruncatedSVD(D)\n",
    "svd.fit(data)\n",
    "\n",
    "svd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "components = svd.transform(data)\n",
    "components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data reduced to 2 components, we can visualize it easily. Each of this rows is a (x,y) point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(components[:,0], components[:,1])\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very messy... We can write a few small functions to pick out a few of our favorite movies and see what's around them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def movie_search(movie_name_input):\n",
    "    condition = movies.movie_title.str.contains(movie_name_input) \n",
    "    return movies[ condition ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "movie_search(\"The Matrix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "movie_search(\"Furious\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def movie_plotter(movie_id, components, x_buffer=3, y_buffer=3):\n",
    "    \n",
    "    # movie_id is the index, we want the 2 components\n",
    "    \n",
    "    x = components[movie_id][0]\n",
    "    y = components[movie_id][1]\n",
    "    \n",
    "    # And we want all of the other movies with close values (range: less and greater)\n",
    "\n",
    "    xs = [x - x_buffer, x + x_buffer]\n",
    "    ys = [y - y_buffer, y + y_buffer]\n",
    "\n",
    "    # Let's plot all the points and then only look at the zoom in that range (xs, ys)\n",
    "    plt.scatter(components[:,0], components[:,1])\n",
    "    plt.title('MOVIES WITH CLOSE COMPONENTS TO: '+ movies['movie_title'].loc[movie_id] +\"\\n\", fontsize=14)\n",
    "    \n",
    "    plt.xlim(xs)\n",
    "    plt.ylim(ys)\n",
    "\n",
    "    # Include titles of movies in that range\n",
    "    \n",
    "    import re\n",
    "    for x, y, title in zip(components[:,0], components[:,1], movies['movie_title']):\n",
    "        if x >= xs[0] and x <= xs[1] and y >= ys[0] and y <= ys[1]:\n",
    "            title_without_symbols = re.sub(r'[^\\w]', ' ', title)\n",
    "            plt.text(x, y, title_without_symbols)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id_to_plot_similar = 7613\n",
    "\n",
    "movie_plotter( id_to_plot_similar, components )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "id_to_plot_similar = 8574\n",
    "\n",
    "movie_plotter( id_to_plot_similar, components, 1,1 )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
