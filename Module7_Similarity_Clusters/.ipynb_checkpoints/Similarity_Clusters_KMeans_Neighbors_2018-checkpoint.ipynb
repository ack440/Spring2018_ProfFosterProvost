{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity, Neighbors, Clustering\n",
    "# and...\n",
    "# Whiskey Analytics!\n",
    "\n",
    "\n",
    "Spring 2018 - Prof. Foster Provost\n",
    "\n",
    "Teacher Assistant: Nicholas Garcia\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.spatial import distance\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "plt.rcParams['figure.figsize'] = 14, 8\n",
    "\n",
    "from dstools import data_tools\n",
    "\n",
    "np.random.seed(36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whiskey Analytics\n",
    "\n",
    "**Given that I discover that I like one whiskey, how can I find another that I might like?**\n",
    "\n",
    "Let's talk about that.....\n",
    "\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "Following that discussion (see Chapter 6 of *Data Science for Business*, if this wasn't interactive for you), we have compiled a scotch whiskey data set. You can find it in `data/scotch.csv`.\n",
    "\n",
    "The data consists of 5 general whiskey attributes, each of which has many possible values:\n",
    "\n",
    "- **Color**: yellow, very pale, pale, pale gold, gold, old gold, full gold, amber, etc.\n",
    "- **Nose**: aromatic, peaty, sweet, light, fresh, dry, grassy, etc.\n",
    "- **Body**: soft, medium, full, round, smooth, light, firm, oily.\n",
    "- **Palate**: full, dry, sherry, big, fruity, grassy, smoky, salty, etc.\n",
    "- **Finish**: full, dry, warm, light, smooth, clean, fruity, grassy, smoky, etc.\n",
    "\n",
    "Let's read it in and take a look. There are a few other features unrelated to the ones above. For this class, we will be dropping them. However, feel free to check them out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/scotch.csv\")\n",
    "\n",
    "data = data.drop([u'age', u'dist', u'score', u'percent', u'region', u'district', u'islay', u'midland', u'spey', u'east', u'west', u'north ', u'lowland', u'campbell', u'islands'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity measures\n",
    "\n",
    "Once we have objects described as data, we can compute the similarity between different objects.  As with most data analytics, the tried-and-true way to represent objects with data is to create a **feature vector** for each object.\n",
    "\n",
    "Thus, each of our whiskeys now is described by its feature vector (68 attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tools.feature_printer(data, 'Aberfeldy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['Aberfeldy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see any other whiskey and its vector, including Foster's favorite, \"Bunnahabhain\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ( data_tools.feature_printer(data, 'Bunnahabhain') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So ... Foster would like to know: What other whiskeys are similar to Bunnahabhain?  Generally, how can we compute similarity between whiskeys?  We've reduced this question to: how can we compute similarity between objects described as feature vectors.\n",
    "\n",
    "There are many similarity measures.  Similarity is often cast as \"closeness\" in some space, as computed by a distance measure.  Often in data science, the terms similarity and distance are used interchangeably (a little strangely to the uninitiated). \n",
    "\n",
    "We'll use the library scipy.spatial.distance available [here](http://docs.scipy.org/doc/scipy/reference/spatial.distance.html)\n",
    "\n",
    "This library has functions to compute the distance between two numeric vectors. In particular, **pdist(X[, metric, p, w, V, VI])**\tcomputes pairwise distances between the observations in n-dimensional space. _Metric parameter: The distance function can be ‘braycurtis’, ‘canberra’, ‘chebyshev’, ‘cityblock’, ‘correlation’, ‘cosine’, ‘dice’, ‘euclidean’, ‘hamming’, ‘jaccard’, ‘kulsinski’, ‘mahalanobis’, ‘matching’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’._\n",
    "\n",
    "Here is a function that will compute the distance using as many metrics as you want:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def whiskey_distance(name, distance_measures, n):\n",
    "    # We want a data frame to store the output\n",
    "    # distance_measures is a list of the distance measures you want to compute (see below)\n",
    "    # n is how many \"most similar\" to report\n",
    "    distances = pd.DataFrame()\n",
    "    \n",
    "    # Find the location of the whiskey we are looking for\n",
    "    whiskey_location = np.where(data.index == name)[0][0]\n",
    "\n",
    "    # Go through all distance measures we care about\n",
    "    for distance_measure in distance_measures:\n",
    "        # Find all pairwise distances\n",
    "        current_distances = distance.squareform(distance.pdist(data, distance_measure))\n",
    "        # Get the closest n elements for the whiskey we care about\n",
    "        most_similar = np.argsort(current_distances[:, whiskey_location])[0:n]\n",
    "        # Append results (a new column to the dataframe with the name of the measure)\n",
    "        distances[distance_measure] = list(zip(data.index[most_similar], current_distances[most_similar, whiskey_location]))\n",
    "    return distances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function `whiskey_distance` to find the distance value of each whiskey against 'Bunnahabhain'. In this case we'll start using Euclidean distance as our metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whiskey_distance('Bunnahabhain', ['euclidean'], 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use more metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whiskey_distance('Bunnahabhain', ['euclidean', 'cityblock', 'cosine', 'jaccard'], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look to the features some of these have and see why they are ranked as being most similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tools.feature_printer(data, 'Bunnahabhain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these 4 measures, this is the \"closest\" (most similar):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tools.feature_printer(data, 'Glenglassaugh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This other example should have fewer features in common:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tools.feature_printer(data, 'Benriach')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Methods\n",
    "\n",
    "Similarity has many uses in data science.  One of the most commonly discussed is clustering: Can we find groups of whiskeys that are similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "There are different ways to find similar groups.  One very common method is Hierarchical Clustering.\n",
    "\n",
    "First let's look at a simple example to illustrate.  Given a set of records (A-F) with two features, we can visualize them on a 2 dimensional surface.  Clustering proceeds as follows.  First consider each point to be its own cluster.  Then, iteratively, group together the closest two clusters.  In the figure, circles were drawn in order of grouping.  The second diagram is a visualization of the hierarchy of groupings, called a \"dendrogram.\"  You can clip it at any point, vertically, and get \"the best\" clustering for a certain number of groups.\n",
    "\n",
    "\n",
    "<img src=\"images/cutting.png\" height=40% width=40%>\n",
    "\n",
    "Here is a visualization of a part of the dendrogram for the whiskey clustering in the book:\n",
    "\n",
    "<img src=\"images/cross_section.png\" height=70% width=70%>\n",
    "\n",
    "***\n",
    "\n",
    "Let's examine the dendrogram(s) for our data, we'll be using the library: **scipy.cluster.hierarchy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets pairwise distances between observations in n-dimensional space.\n",
    "dists = pdist(data, metric=\"cosine\")\n",
    "\n",
    "# This scipy's function performs hierarchical/agglomerative clustering on the condensed distance matrix y.\n",
    "links = linkage(dists, method='average')\n",
    "\n",
    "# Now we want to plot those 'links' using \"dendrogram\" function\n",
    "plt.rcParams['figure.figsize'] = 32, 16\n",
    "\n",
    "den = dendrogram(links)\n",
    "\n",
    "plt.xlabel('Samples',fontsize=18)\n",
    "plt.ylabel('Distance',fontsize=18)\n",
    "plt.xticks(rotation=90,fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use other measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets pairwise distances between observations in n-dimensional space.\n",
    "dists = pdist(data, metric=\"euclidean\")\n",
    "\n",
    "# This scipy's function performs hierarchical/agglomerative clustering on the condensed distance matrix y.\n",
    "links = linkage(dists, method='average')\n",
    "\n",
    "# Now we want to plot those 'links' using \"dendrogram\" function\n",
    "plt.rcParams['figure.figsize'] = 32, 16\n",
    "\n",
    "den = dendrogram(links)\n",
    "\n",
    "plt.xlabel('Samples',fontsize=18)\n",
    "plt.ylabel('Distance',fontsize=18)\n",
    "plt.xticks(rotation=90,fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[74:75]\n",
    "#data[1:2]\n",
    "#data[30:31]\n",
    "#data[108:109]\n",
    "#print(np.where(data.index=='Bunnahabhain')[0])\n",
    "#data[18:19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common to cut dendrograms at a particular height and to then use the resulting clusters. \n",
    "\n",
    "<img src=\"images/clustering.png\" height=90% width=90%>\n",
    "\n",
    "[That's David Whishart who wrote a well-known book based on clustering whiskeys.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans\n",
    "\n",
    "Another method for finding clusters is to use the KMeans algorithm to find a set of $k$ clusters. Here, unlike in hierarchical clustering, we define the number of clusters in advance. We'll use the library **sklearn.cluster**\n",
    "\n",
    "Here is a nice illustrated example: http://util.io/k-means  (but it looks like the interactivity is broken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k_clusters = 6\n",
    "\n",
    "## Fit clusters like in our previous models/transformations/standarization \n",
    "## (e.g. Logistic, Vectorization,...)\n",
    "\n",
    "model = KMeans(k_clusters)\n",
    "model.fit(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What clusters do we get? Let's get \"predictions\" of the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Records in our dataset (rows): \", len(data.index))\n",
    "print (\"Then we predict one cluster per record, which means length of: \", len(model.predict(data)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = model.predict(data)\n",
    "\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(zip(data.index,model.predict(data))), columns=['Whiskey','Cluster_predicted']) [0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put each cluster into its own column!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cluster_listing = {}\n",
    "for cluster in range(k_clusters):\n",
    "    cluster_listing['Cluster ' + str(cluster)] = [''] * 109\n",
    "    where_in_cluster = np.where(clusters == cluster)[0]\n",
    "    cluster_listing['Cluster ' + str(cluster)][0:len(where_in_cluster)] = data.index[where_in_cluster]\n",
    "\n",
    "# Print clusters\n",
    "pd.DataFrame(cluster_listing).loc[0:np.max(np.bincount(clusters)) - 1,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we name or describe these clusters? \n",
    "\n",
    "[That's a question for you!]\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's take a look at the results of a particular clustering from Lapointe and Legendre's *A Classification of Pure Malt Scotch Whiskies*. In this clustering, they create 12 clusters A through L. Let's take cluster J as an example and built a decision tree that will classifier all whiskies as either belonging to J or not belonging to J.\n",
    "\n",
    "<img src=\"images/cluster_tree.png\" height=50% width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok.\n",
    "\n",
    "Now just for illustration, let's take a look at a different data set, that only has two features. This will make visualizing what's going on much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function returns 2 columns of data and the Y-target\n",
    "\n",
    "X, Y = data_tools.make_cluster_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the target Y, let's plot the data and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:, 1], s=20)\n",
    "plt.xlabel(\"Feature 1\",fontsize=18)\n",
    "plt.ylabel(\"Feature 2\",fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to what we did above, we can apply **KMeans** to this data. Let's try a few different values for the number of clusters $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans\n",
    "model = KMeans(2)\n",
    "model.fit(X)\n",
    "\n",
    "# Predict clusters\n",
    "clusters = model.predict(X)\n",
    "\n",
    "# Plot the same points but set two different colors (based on the cluster's results)\n",
    "\n",
    "plt.scatter(X[:,0], X[:, 1], color=data_tools.colorizer(clusters), linewidth=0, s=20)\n",
    "plt.xlabel(\"Feature 1\",fontsize=18)\n",
    "plt.ylabel(\"Feature 2\",fontsize=18)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans\n",
    "model = KMeans(3)\n",
    "model.fit(X)\n",
    "\n",
    "# Predict clusters\n",
    "clusters = model.predict(X)\n",
    "\n",
    "# Plot the same points but now set 3 different colors (based on the cluster's results)\n",
    "\n",
    "plt.scatter(X[:,0], X[:, 1], color=data_tools.colorizer(clusters), linewidth=0, s=20)\n",
    "plt.xlabel(\"Feature 1\",fontsize=18)\n",
    "plt.ylabel(\"Feature 2\",fontsize=18)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction via Similarity\n",
    "\n",
    "Now, what if we have a target variable to estimate/predict and labels for a training set?  We can do prediction directly using similarity.\n",
    "\n",
    "For example, in this 2 dimensional data that we're using, we have five numerical **labels, 0 through 4**. One way to use similarity to build a predictor<sup>&dagger;</sup> is to use a **Nearest Neighbor algorithm**.  The idea is: to predict the value of the target variable for a data item, first find the most similary (closest) training data items.  The **k-Nearest-Neighbor** or **kNN** algorithm chooses the closest `k` data points.  Then, gather the values of the target variable for them, and then combine them somehow.  So, to classify, one might combine them by having them vote their classes.  (How would you combine to compute probability estimates?  How would you combine for a regression problem?)\n",
    "\n",
    "<sup>&dagger;</sup>There's an interesting question as to whether we're actually building a *model* here.\n",
    "\n",
    "Let's start by splitting our X and Y data into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=.75)\n",
    "\n",
    "pd.DataFrame( list(zip(X[:,0],X[:, 1],Y_train)), columns=['Feature1','Feature2','Target']) [0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's so our scatter plot with the true labels first. This is another option to plot different colors, using pylab!!\n",
    "\n",
    "[Other maps of colors here..](http://matplotlib.org/examples/color/colormaps_reference.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pylab\n",
    "\n",
    "plt.scatter( X[:,0], X[:, 1], c = Y, linewidth=0, s=20, cmap = pylab.cm.brg )\n",
    "plt.xlabel(\"Feature 1\",fontsize=20)\n",
    "plt.ylabel(\"Feature 2\",fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try setting the number of neighbors to use, $k$, to a few different values and look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# KNN\n",
    "model = KNeighborsClassifier(50)\n",
    "model.fit(X_train, Y_train)\n",
    "data_tools.Decision_Surface(X, Y, model, cell_size=.1, surface=True, points=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# KNN\n",
    "model = KNeighborsClassifier(5)\n",
    "model.fit(X_train, Y_train)\n",
    "data_tools.Decision_Surface(X, Y, model, cell_size=.05, surface=True, points=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# KNN\n",
    "model = KNeighborsClassifier(5)\n",
    "model.fit(X_train, Y_train)\n",
    "data_tools.Decision_Surface(X, Y, model, cell_size=.05, surface=True, points=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that as we make $k$ smaller, we get many smaller blobs all bunched together. What happens when we get down to $k=1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we have 5 classes, we can still use the evaluation metrics we have already learned about. Accuracy should be straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for k in [1, 10, 50, 100, 1000, 2000]:\n",
    "    model = KNeighborsClassifier(k)\n",
    "    model.fit(X_train, Y_train)\n",
    "    print (\"Accuracy with k = %d is %.3f\" % (k, metrics.accuracy_score(Y_test, model.predict(X_test))) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at AUC; we can do this evaluating each label (0 to 4) versus the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [1, 10, 50, 100, 1000]:\n",
    "    \n",
    "    model = KNeighborsClassifier(k)\n",
    "    model.fit(X_train, Y_train)\n",
    "    probabilities = model.predict_proba(X_test)\n",
    "\n",
    "    print(\"KNN with k = %d\" % k)\n",
    "    aucs = 0\n",
    "    for i in range(5):\n",
    "        auc = metrics.roc_auc_score(Y_test == i, probabilities[:,i])\n",
    "        aucs += auc\n",
    "        print(\"   AUC for label %d vs. rest = %.3f\" % (i, auc))\n",
    "        \n",
    "    print(\"   Average AUC = %.3f\\n\" % (aucs / 5.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
